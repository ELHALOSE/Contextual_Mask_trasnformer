{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7bHKGiOc5fD"
      },
      "source": [
        "Download the News Category dataset from here: https://metatext.io/datasets/news-category-dataset then upload it to your google drive in a folder called \"datasets\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73O-B5LcUOHh",
        "outputId": "48bc7b49-fe4c-4928-ddd1-a6a759238a99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K95N7-KwdIEm"
      },
      "outputs": [],
      "source": [
        "file = \"/content/gdrive/MyDrive/datasets/news_category_dataset_sample.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDHoiPPCd3u-",
        "outputId": "446eef39-1121-44e5-e028-201bc7809f00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "124\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "def process_file(file_path):\n",
        "  news = []\n",
        "  with open(file_path, 'r') as file:\n",
        "          for line in file:\n",
        "              json_object = json.loads(line)\n",
        "              news.append(json_object)\n",
        "  return news\n",
        "\n",
        "news = process_file(file)\n",
        "print(len(news))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TT74ji6Wd-J6",
        "outputId": "1817ad83-71ee-4ad4-9f78-d91a69fad568"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'link': 'https://www.huffpost.com/entry/belk-worker-found-dead-columbiana-centre-bathroom_n_632c5f8ce4b0572027b0251d',\n",
              " 'headline': 'Cleaner Was Dead In Belk Bathroom For 4 Days Before Body Found: Police',\n",
              " 'category': 'U.S. NEWS',\n",
              " 'short_description': 'The 63-year-old woman was seen working at the South Carolina store on Thursday. She was found dead Monday after her family reported her missing, authorities said.',\n",
              " 'authors': '',\n",
              " 'date': '2022-09-22'}"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "news[5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3R5A54jZfRF7",
        "outputId": "558bd12b-54b9-4036-cae1-24e719169562"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Cleaner Was Dead In Belk Bathroom For 4 Days Before Body Found: Police || The 63-year-old woman was seen working at the South Carolina store on Thursday. She was found dead Monday after her family reported her missing, authorities said.',\n",
              " 'U.S. NEWS')"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "text_cat_pairs = []\n",
        "\n",
        "for news_item in news:\n",
        "    headline = news_item.get(\"headline\")\n",
        "    short_description = news_item.get(\"short_description\")\n",
        "    text = headline + \" || \" + short_description\n",
        "    category = news_item.get(\"category\")\n",
        "    text_cat_pairs.append((text, category))\n",
        "\n",
        "text_cat_pairs[5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbpNSOaFf1wg"
      },
      "outputs": [],
      "source": [
        "# iterate over text_cat_pairs, which is a list of tuples. Looks at the second element in the tuple. Generate a label_to_index dict and an index_to_label dict from those labels.\n",
        "\n",
        "label_to_index = {}\n",
        "index_to_label = {}\n",
        "i = 0\n",
        "\n",
        "for (_, label) in text_cat_pairs:\n",
        "    if label not in label_to_index:\n",
        "        label_to_index[label] = i\n",
        "        index_to_label[i] = label\n",
        "        i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mg2BVW8hVeZC",
        "outputId": "88a0fa32-2470-4bb2-df85-4456fab38247"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'U.S. NEWS': 0,\n",
              " 'COMEDY': 1,\n",
              " 'PARENTING': 2,\n",
              " 'WORLD NEWS': 3,\n",
              " 'CULTURE & ARTS': 4,\n",
              " 'TECH': 5,\n",
              " 'SPORTS': 6,\n",
              " 'ENTERTAINMENT': 7,\n",
              " 'POLITICS': 8,\n",
              " 'WEIRD NEWS': 9,\n",
              " 'ENVIRONMENT': 10,\n",
              " 'EDUCATION': 11,\n",
              " 'CRIME': 12,\n",
              " 'SCIENCE': 13}"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "label_to_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oy7LvT2BVz8t",
        "outputId": "9ca75d44-8b3f-45b4-b933-941b5d447b8e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'U.S. NEWS',\n",
              " 1: 'COMEDY',\n",
              " 2: 'PARENTING',\n",
              " 3: 'WORLD NEWS',\n",
              " 4: 'CULTURE & ARTS',\n",
              " 5: 'TECH',\n",
              " 6: 'SPORTS',\n",
              " 7: 'ENTERTAINMENT',\n",
              " 8: 'POLITICS',\n",
              " 9: 'WEIRD NEWS',\n",
              " 10: 'ENVIRONMENT',\n",
              " 11: 'EDUCATION',\n",
              " 12: 'CRIME',\n",
              " 13: 'SCIENCE'}"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "index_to_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMetlxxkVghN",
        "outputId": "a916a820-455d-4f8c-f03a-6ff6476da568"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "U.S. NEWS\n",
            "tensor(0)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "def convert_labels(label):\n",
        "  return torch.tensor(label_to_index[label])\n",
        "\n",
        "\n",
        "labels = [cat for (text, cat) in text_cat_pairs]\n",
        "print(labels[5])\n",
        "print(convert_labels(labels[5]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdDwBSjuX4mi",
        "outputId": "816fd1c2-aefa-45da-c3b2-326b240eb3e4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([124])"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "labels = [convert_labels(label) for label in labels]\n",
        "stacked_tensors_y = torch.stack(labels)\n",
        "stacked_tensors_y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8bDhor8Yyws"
      },
      "outputs": [],
      "source": [
        "stacked_tensors_y = stacked_tensors_y.long()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXjlQrmFZcYx"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# Load pre-trained model tokenizer and embedding model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "embedding_model = BertModel.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "WK1C29TTXKJ8",
        "outputId": "38ecfb3c-e98f-450a-d49c-10b2c09747fc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Woman Who Called Cops On Black Bird-Watcher Loses Lawsuit Against Ex-Employer || Amy Cooper accused investment firm Franklin Templeton of unfairly firing her and branding her a racist after video of the Central Park encounter went viral.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "ex = text_cat_pairs[4][0]\n",
        "ex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zs3upvENXOsL",
        "outputId": "36ffd64b-e830-4232-814f-31d6e48316e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', 'woman', 'who', 'called', 'cops', 'on', 'black', 'bird', '-', 'watch', '##er', 'loses', 'lawsuit', 'against', 'ex', '-', 'employer', '|', '|', 'amy', 'cooper', 'accused', 'investment', 'firm', 'franklin', 'temple', '##ton', 'of', 'unfair', '##ly', 'firing', 'her', 'and', 'branding', 'her', 'a', 'racist', 'after', 'video', 'of', 'the', 'central', 'park', 'encounter', 'went', 'viral', '.', '[SEP]']\n"
          ]
        }
      ],
      "source": [
        "tokens = tokenizer.encode(ex)\n",
        "print([tokenizer.decode([t]) for t in tokens])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Wg88AX_ZrGy"
      },
      "outputs": [],
      "source": [
        "def embed_sentence(sentence):\n",
        "  inputs = tokenizer(sentence, return_tensors='pt')\n",
        "  with torch.no_grad():\n",
        "    outputs = embedding_model(**inputs)\n",
        "    embeddings = outputs.last_hidden_state\n",
        "  return embeddings.view(embeddings.size(1), -1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-tPPmPhZK13",
        "outputId": "adffd1b1-8f47-432c-9133-e01bd9284273"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "48"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ],
      "source": [
        "len(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57Xz5OKQao_Z",
        "outputId": "59a4e80a-ca93-4844-e0d6-3c8964a9085c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([48, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ],
      "source": [
        "embedded_example = embed_sentence(ex)\n",
        "embedded_example.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JC-C2YOHbA3R",
        "outputId": "4949eaa1-ac30-40ac-9e3c-52c760dcc7f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([45, 768])\n"
          ]
        }
      ],
      "source": [
        "embedded_sentences = [embed_sentence(text) for (text, cat) in text_cat_pairs]\n",
        "print(embedded_sentences[-1].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhdzR6jtZz0n",
        "outputId": "1f37b98f-ddac-4deb-d9d5-14ac061d4c2a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "124"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ],
      "source": [
        "len(embedded_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0pF7zywbauR",
        "outputId": "33f473cc-0bd8-443a-cc53-0a08d37ec5da"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([124, 69, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ],
      "source": [
        "# Find the maximum length among the embedded sentences\n",
        "max_len = max([x.shape[0] for x in embedded_sentences])\n",
        "\n",
        "# Pad the shorter sentences with zeros\n",
        "padded_sentences = []\n",
        "for sentence in embedded_sentences:\n",
        "  padding_length = max_len - sentence.shape[0]\n",
        "  if padding_length == 0:\n",
        "    padded_sentences.append(sentence)\n",
        "  else:\n",
        "    padding = torch.zeros(padding_length, sentence.shape[1])\n",
        "    padded_sentence = torch.cat((sentence, padding), dim=0)\n",
        "    padded_sentences.append(padded_sentence)\n",
        "\n",
        "# Stack the padded tensors\n",
        "stacked_tensors_x = torch.stack(padded_sentences)\n",
        "stacked_tensors_x.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class TransformerClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes, num_heads=6, dropout=0.1):\n",
        "        super(TransformerClassifier, self).__init__()\n",
        "\n",
        "        # Transformer layer\n",
        "        self.transformer_encoder = torch.nn.MultiheadAttention(input_dim, num_heads,\n",
        "                                                               dropout=dropout, bias=False,\n",
        "                                                               kdim=input_dim, vdim=input_dim,\n",
        "                                                               batch_first=True)\n",
        "        self.norm = nn.LayerNorm(input_dim)\n",
        "        # Fully connected Head\n",
        "        self.fc = nn.Linear(input_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Transformer Encoder\n",
        "        output, _ = self.transformer_encoder(x, x, x, need_weights=False)\n",
        "        # apply layer normalization\n",
        "        output = self.norm(output)\n",
        "        # take the average over all attention (hidden) states\n",
        "        output = torch.mean(output, dim=1)\n",
        "        # Fully Connected Layer for Classification\n",
        "        output = self.fc(output)\n",
        "\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "OV4crUuFBqnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above, we used only one attention layer. To stack multiple attention layers, we can use the TransformerEncoder class from transformers instead."
      ],
      "metadata": {
        "id": "hG6J5kb8DYra"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFIBAG_Th3RM",
        "outputId": "acbaf006-f4c9-4bf3-afcc-57cdab6f00e1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(768, 14)"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ],
      "source": [
        "embedding_dimensionality = stacked_tensors_x.shape[-1]\n",
        "num_classes = len(label_to_index)\n",
        "embedding_dimensionality, num_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABjE7ZMFiLrf"
      },
      "outputs": [],
      "source": [
        "input_size = embedding_dimensionality\n",
        "output_size = num_classes\n",
        "model = TransformerClassifier(input_size, output_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPh5vrvticsY",
        "outputId": "bf591d6c-284a-4bc6-e205-d0a43382042b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([124, 14])"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ],
      "source": [
        "output = model(stacked_tensors_x)\n",
        "output.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ybSd5g0if5S"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "loss = criterion(output, stacked_tensors_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQ_acKGzkJpi",
        "outputId": "2a867a9d-0529-454e-c8c3-61201cd9f699"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.8244, grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ],
      "source": [
        "loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFj9Pg0mkKrd"
      },
      "outputs": [],
      "source": [
        "loss.backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0ddJ-ZWkdJQ"
      },
      "outputs": [],
      "source": [
        "from torch import optim\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.005)\n",
        "\n",
        "# take an optimization step to update weights\n",
        "optimizer.step()\n",
        "# delete the accumulated gradients after each pass\n",
        "optimizer.zero_grad()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Voc8q_o3nQop"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}