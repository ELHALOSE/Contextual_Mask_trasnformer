{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "file= \"/content/Hitchhiker's-Guide-to-the-Galaxy,-The.txt\""
      ],
      "metadata": {
        "id": "PUNhhWDqwmGX"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_file(file_path):\n",
        "  paragraphs = []\n",
        "  with open(file_path, 'r') as file:\n",
        "          for line in file:\n",
        "            if len(line.strip()) < 2:\n",
        "              continue\n",
        "            #line = \"[START] \" + line.strip() + \" [END]\"\n",
        "            paragraphs.append(line.strip())\n",
        "  return paragraphs\n",
        "\n",
        "content = process_file(file)\n",
        "print(len(content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "WDGE_WGTu42_",
        "outputId": "0cd7675f-1f9b-4704-ed3a-841ac54a5176"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4688\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "content[11]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "OhqTkpm1yD2_",
        "outputId": "761657ff-9535-4b1b-fcb3-c6a6172d94c1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3rd Revised Draft'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "from transformers import BertTokenizer, BertForMaskedLM"
      ],
      "metadata": {
        "id": "hdkXiVG2P00n"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerLM(nn.Module):\n",
        "    def __init__(self, model_name=\"bert-base-uncased\", mask_token=\"[MASK]\"):\n",
        "        super(TransformerLM, self).__init__()\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "        self.model = BertForMaskedLM.from_pretrained(model_name)\n",
        "\n",
        "        # Add [MASK] token to tokenizer and vocabulary if not already present\n",
        "        if mask_token not in self.tokenizer.get_vocab():\n",
        "            self.tokenizer.add_special_tokens({\"additional_special_tokens\": [mask_token]})\n",
        "            self.model.resize_token_embeddings(len(self.tokenizer))\n",
        "\n",
        "        self.mask_token_id = self.tokenizer.convert_tokens_to_ids(mask_token)\n",
        "\n",
        "    def mask_tokens(self, input_ids, mask_probability=0.2):\n",
        "        \"\"\"\n",
        "        Masks tokens in the input with the given probability and stores the original tokens.\n",
        "\n",
        "        Args:\n",
        "            input_ids (torch.Tensor): Input IDs tensor.\n",
        "            mask_probability (float): Probability of masking each token.\n",
        "\n",
        "        Returns:\n",
        "            masked_input_ids (torch.Tensor): Input IDs with tokens replaced by [MASK].\n",
        "            masked_indices (list): Indices of masked tokens.\n",
        "            correct_labels (torch.Tensor): The original token IDs for the masked positions.\n",
        "            masked_sentence (list): List of tokens with [MASK] inserted.\n",
        "        \"\"\"\n",
        "        masked_input_ids = input_ids.clone()\n",
        "\n",
        "        # Exclude special tokens like [CLS] and [SEP] from being masked\n",
        "        mask_candidates = (input_ids != self.tokenizer.cls_token_id) & (input_ids != self.tokenizer.sep_token_id)\n",
        "        mask_indices = torch.nonzero(mask_candidates, as_tuple=True)[1].tolist()\n",
        "\n",
        "        # Choose 20% of the tokens to mask, or 1 token if there are fewer than 5 tokens\n",
        "        num_tokens_to_mask = max(1, int(len(mask_indices) * mask_probability))\n",
        "        selected_indices = random.sample(mask_indices, num_tokens_to_mask)\n",
        "\n",
        "        correct_labels = masked_input_ids.clone()\n",
        "\n",
        "        # Mask the selected tokens and build the masked sentence list\n",
        "        masked_sentence = self.tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n",
        "        for idx in selected_indices:\n",
        "            masked_input_ids[0, idx] = self.mask_token_id  # Replace with [MASK]\n",
        "            masked_sentence[idx] = self.tokenizer.mask_token  # Insert [MASK] in the sentence\n",
        "\n",
        "        return masked_input_ids, selected_indices, correct_labels[0, selected_indices], masked_sentence\n",
        "\n",
        "    def forward(self, input_paragraph):\n",
        "        \"\"\"\n",
        "        Performs a forward pass on the model.\n",
        "\n",
        "        Args:\n",
        "            input_paragraph (str): The input paragraph as a string.\n",
        "\n",
        "        Returns:\n",
        "            loss (torch.Tensor): Computed loss for the masked tokens.\n",
        "            predictions (torch.Tensor): Model predictions for the masked tokens.\n",
        "            predicted_tokens (list): List of predicted tokens for the masked positions.\n",
        "            masked_sentence (list): Masked sentence with [MASK] token inserted.\n",
        "        \"\"\"\n",
        "        # Tokenize input paragraph\n",
        "        tokenized = self.tokenizer(input_paragraph, return_tensors=\"pt\")\n",
        "        input_ids = tokenized.input_ids\n",
        "\n",
        "        # Mask tokens\n",
        "        masked_input_ids, masked_indices, correct_labels, masked_sentence = self.mask_tokens(input_ids)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = self.model(masked_input_ids)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Extract logits corresponding to masked tokens\n",
        "        masked_logits = logits[0, masked_indices]\n",
        "\n",
        "        # Compute loss\n",
        "        loss_fn = nn.CrossEntropyLoss()\n",
        "        loss = loss_fn(masked_logits, correct_labels)\n",
        "\n",
        "        # Get predicted token IDs for masked positions\n",
        "        predicted_indices = torch.argmax(masked_logits, dim=-1).tolist()\n",
        "\n",
        "        # Convert predicted token IDs to tokens\n",
        "        predicted_tokens = self.tokenizer.convert_ids_to_tokens(predicted_indices)\n",
        "\n",
        "        return loss, predicted_tokens, masked_sentence\n"
      ],
      "metadata": {
        "id": "fDSOH1dbIZkx"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize model\n",
        "transformer_lm = TransformerLM()\n",
        "\n",
        "# Example paragraph\n",
        "paragraph = \"dolphins leap over and interact with the opening titles.\"\n",
        "\n",
        "# Forward pass\n",
        "loss, predicted_tokens, masked_sentence = transformer_lm(paragraph)\n",
        "\n",
        "print(f\"Original Sentence: {paragraph.split()}\")\n",
        "print(f\"Masked Sentence: {masked_sentence}\")\n",
        "print(f\"Predicted Tokens for [MASK]: {predicted_tokens}\")\n",
        "print(f\"Loss: {loss.item()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "bqvibr1qMpXY",
        "outputId": "30049e81-baf2-43e8-86f0-11450c769527"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Sentence: ['dolphins', 'leap', 'over', 'and', 'interact', 'with', 'the', 'opening', 'titles.']\n",
            "Masked Sentence: ['[CLS]', 'dolphins', 'leap', 'over', 'and', '[MASK]', 'with', 'the', '[MASK]', 'titles', '.', '[SEP]']\n",
            "Predicted Tokens for [MASK]: ['two', 'play']\n",
            "Loss: 7.281673431396484\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3RUxEBRdOq4e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}